{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers para Martin Fierro\n",
    "\n",
    "Los dos principales objetivos de este tutorial son mostrar c√≥mo es una arquitectura transformer y c√≥mo usar las herramientas provistas por [Hugging Face](https://huggingface.co/). En este tutorial usaremos una implementaci√≥n de [GPT2](https://openai.com/blog/better-language-models/) en espa√±ol para generar (una vez m√°s) texto similar al Mart√≠n Fierro, que se puede descargar [aqu√≠](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt).\n",
    "\n",
    "Cr√©ditos a [Jay Alammar](https://jalammar.github.io/illustrated-gpt2/) por las im√°genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import wget\n",
    "import unicodedata, re\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT2 es un modelo de generaci√≥n de texto basado en transformers. A diferencia de BERT u otros transformers, GPT2 est√° basado en sucesivas instancias de decoders\n",
    "\n",
    "<img src=\"images/gpt-2-transformer-xl-bert-3.png\"\n",
    "     alt=\"GPT2 Decoder Architecture\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "<img src=\"images/gpt2_function.gif\"\n",
    "     alt=\"GPT2 Text Generation Function\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "**Recursos:**\n",
    "\n",
    "[The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Usando transformers de Hugging Face\n",
    "\n",
    "La librer√≠a transformers permite usar modelos ya entrenados en un pipeline y nos facilita funciones que procesan el input, lo tokenizan y hace el forward pass para generar las predicciones.\n",
    "\n",
    "Dada una tarea, pipeline descarga un modelo y tokenizador apropiados para la tarea. En este caso, especificamos un modelo preentrenado de GPT2-small en idioma espa√±ol.\n",
    "\n",
    "<img src=\"images/gpt2_sizes.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "<img src=\"images/gpt2_decoder_stacks.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "\n",
    "Para nuestra tarea en particular, pipeline tomar√° un texto e internamente se encargar√° del tokenizado de la oraci√≥n y de generar texto hasta el *max_length* indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=\"datificate/gpt2-small-spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'Yo he visto en esa milonga muchos Gefes con estancia,'\n",
    "output = generator(input_sentence, max_length=50)\n",
    "print('Generated Text: %s...' % input_sentence)\n",
    "print('... ', output[0]['generated_text'][len(input_sentence):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar otras tareas de ü§ó Transformers, [aqu√≠](https://huggingface.co/transformers/task_summary.html) hay una lista completa de todas las tareas que se implementan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Importando de ü§ó Transformers\n",
    "\n",
    "Lo primero es importar el modelo con el cu√°l trabajaremos. En nuestro caso usaremos [GPT2-Small en espa√±ol de Datificate](https://huggingface.co/datificate/gpt2-small-spanish) (pueden jugar con el modelo desde la misma p√°gina de ü§ó). \n",
    "\n",
    "Nuestrea tarea de inter√©s es hacer Language Modeling del Mart√≠n Fierro, por lo cual importamos **GPT2LMHeadModel**, que es una implementaci√≥n de GPT2 con la √∫ltima capa lineal para retornar los logits del tama√±o del vocabulario.\n",
    "\n",
    "Como el modelo *datificate/gpt2-small-spanish* fu√© entrenado en tensorflow, usamos el flag *from_tf=True* para asegurarnos que importe los pesos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('datificate/gpt2-small-spanish', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es descargar un tokenizador. El tokenizador har√° por nosotres el trabajo de separar el texto en palabras y convertirlas en sus correspondientes ids dentro del vocabulario. Al igual que con el modelo, ü§ó provee tokenizers ya implementados para sus correspondientes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('datificate/gpt2-small-spanish', add_prefix_space=True)\n",
    "\n",
    "tokenized_seq = tokenizer(\"Yo nunca habia escuchado hablar de Innsmouth hasta\")\n",
    "print('inputs_ids: {}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask: {}'.format(tokenized_seq['attention_mask']))\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_seq['input_ids'])\n",
    "print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizer se puede encargar por nosotres de manejar temas de padding y truncado entre otros. Para m√°s info en las funciones del tokenizers, mirar [aqu√≠](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "El tokenizer tambi√©n puede procesar m√∫ltiples oraciones al mismo tiempo y retornar tensores para el framework que estemos usando de base, sea este TensorFlow 2.0 ('*tf*') o PyTorch ('*pt*')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Yo nunca habia escuchado hablar de Innsmouth hasta\",\n",
    "             \"Hacia el oeste de Arkham las monta√±as se levantaban ind√≥mitas y en sus entra√±as\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seq = tokenizer(sentences,\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\")\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso el tokenizer puede manejar listas de palabras con solo agregar el par√°metro *is_split_into_words=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seq = tokenizer([sent.split() for sent in sentences],\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\",\n",
    "                          is_split_into_words=True)\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Finetune con PyTorch nativo\n",
    "\n",
    "Para entrenar con PyTorch, necesitamos crear una instancia de un Dataset de PyTorch. Esta vez, podemos usar el tokenizer que ya importamos para hacer el encoding de las palabras.\n",
    "\n",
    "Por simplicidad, dividiremos el el conjunto de datos en palabras y usaremos listas de palabras de tama√±o fijo.\n",
    "\n",
    "Para entrenamiento nativo en Tensorflow 2, ver [aqu√≠](https://huggingface.co/transformers/training.html#fine-tuning-in-native-tensorflow-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MartinFierroDatasetGPT2(Dataset):\n",
    "    def __init__(self, textdata, maxlen, tokenizer):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        # Get ourselves a list of words so we can iterate\n",
    "        split_text = textdata.split()\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        self.sentences = {}\n",
    "        for idx, i in enumerate(range(0, len(split_text) - maxlen - 1, maxlen)):\n",
    "            self.sentences[idx] = split_text[i: i + maxlen]\n",
    "\n",
    "        # You need to activate padding in order to\n",
    "        # return tensors\n",
    "        self.data = tokenizer(list(self.sentences.values()),\n",
    "                              padding=True,\n",
    "                              is_split_into_words=True,\n",
    "                              return_tensors=\"pt\")\n",
    "        \n",
    "        self.length = len(self.sentences)\n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo e instanciamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))\n",
    "\n",
    "train_dataset = MartinFierroDatasetGPT2(text, 50, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos nuestro modelo en modo train y seteamos el dispositivo donde vamos a ejecutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos de transformers la implementacion de [AdamW](https://arxiv.org/abs/1711.05101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestro dataloader con el dataset del Mart√≠n Fierro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_config = {'dataset': train_dataset,\n",
    "                     'batch_size': 8,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "dataloader = DataLoader(**dataloader_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hacemos un forward pass por todo el dataset (i.e. entrenamos por una epoch). El modelo importado de ü§ó transformers ya se encarga de calcular la loss para la tarea que necesitamos (en este caso es un LM) en el forward pass si le pasamos el argumento *labels*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stream = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "for i, sample in stream:\n",
    "    input_ids = sample['input_ids'].to(device)\n",
    "    attention_mask = sample['attention_mask'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids)\n",
    "    optimizer.zero_grad()\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    stream.set_postfix({'loss': loss.detach().cpu().numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenando con Transformers Trainers\n",
    "\n",
    "ü§ó tambi√©n provee sus propios m√©todos para entrenar. En este caso:\n",
    "  - TrainingArgument: nos genera una configuraci√≥n para un entrenamiento\n",
    "  - Trainer: clase que se encargar√° del entrenamiento por nosotres\n",
    "  \n",
    "Importamos el Data Collator necesario para nuestra tarea y pasamos los argumentos necesario para cada objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# We're not training with MLM, so we must set mlm=False\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-fierro',      # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=1,    # batch size for evaluation\n",
    "    warmup_steps=1,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset (THE SAME AS BEFORE)\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenar es tan simple como llamar al m√©todo train() del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 4: Usando nuestro nuevo modelo en un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_sentence = 'Yo he visto en esa milonga muchos Gefes con estancia,'\n",
    "output = generator(input_sentence, max_length=50)\n",
    "print('Generated Text: %s...' % input_sentence)\n",
    "print('... ', output[0]['generated_text'][len(input_sentence):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios adicionales\n",
    "\n",
    "Juegen :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
