{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs para Martín Fierro [Versión 2020]\n",
    "\n",
    "El objetivo de los ejercicios en este tutorial es mostrar el impacto de algunas decisiones de diseño en la implementación de las redes neuronales, particularmente las recurrentes. Como ejemplo veremos una implementación de la red RNN para generación de lenguaje basada en caracteres de [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Para entrenarla utilizaremos un fragmento del Martín Fierro que pueden descargar [aquí](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt). Para un entrenamiento más complejo, pueden utilizar las obras completas de borges, disponibles en [este link](https://drive.google.com/file/d/0B4remi0ZCiqbUFpTS19pSmVFYkU/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leeremos el dataset del archivo de texto y lo preprocesaremos para disminuir la viariación de caracteres. Normalizaremos el formato unicos, elminaremos espacios y transformaremos todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('martin_fierro.txt'):\n",
    "    wget.download('https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, contaremos la cantidad de caracteres únicos presentes en el texto, y le asignaremos a cada uno un índice único y secuencial. Este índice será utilizado luego para crear las representaciones one-hot encoding de los caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print('Total chars: %d' % len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "La tarea de Language Modiling (**LM**) es aprender $P_{\\theta}$ parametrizada por $\\theta$ para determinar $P_{\\theta}(x|x_1,...x_t)$, donde $x$ puede ser caracter o palabra.\n",
    "\n",
    "$LM(x, x_1, ..., x_t) = P_{\\theta}(x | x_1,...x_t)$\n",
    "\n",
    "Para la tarea de generación, la tarea de **LM** es la ideal: dada una secuencia de entrada $x_1, ..., x_t$, podemos predecir la palabra de mayor probabilidad según nuestra probabilidad $P$.\n",
    "\n",
    "$GenerationLM(x_1, ..., x_t) = max_{x} P_{\\theta, x_1,...x_t}(x)$\n",
    "\n",
    "Otra opción, sería sortear la siguiente palabra con respecto a la distribución generada, es decir, samplear $x \\sim P_{\\theta, x_1,...x_t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Esqueleto de la red neuronal\n",
    "\n",
    "Lo primero que debemos pensar es cómo será la arquitectura de nuestra red para resolver la tarea deseada. En esta sección crearemos el modelo sequencial con PyTorch que representará nuestra red. En los pasos siguientes, implementaremos las transformaciones del corpus, por lo que en este paso pueden asumir cualquier formato en los datos de entrada.\n",
    "\n",
    "Para poder implementar el modelo debemos responder las siguientes preguntas:\n",
    "  - ¿Es una red one-to-one, one-to-many, many-to-one o many-to-many?\n",
    "  - ¿Cuál es el formato de entrada y de salida de la red? ¿Cuál es el tamaño de las matrices (tensores) de entrada y de salida?\n",
    "  - Luego de que la entrada pasa por la capa recurrente, ¿qué tamaño tiene el tensor?\n",
    "  - ¿Cómo se conecta la salida de la capa recurrente con la capa densa que realiza la clasificación?\n",
    "  - ¿Cuál es el loss apropiado para este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos los módulos que necesitaremos para implementar nuestra red:\n",
    "  - torch: acceso a todo el framework\n",
    "  - torch.nn: nos da acceso a capas ya implementadas y a la clase Module para instanciar y crear nuestra red\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if we have a GPU available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_layer,\n",
    "                 num_layers=1, dropout=0., bias=True,\n",
    "                 bidirectional=False):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Set our LSTM parameters\n",
    "        self.lstm_config = {'input_size': input_size,\n",
    "                            'hidden_size': hidden_layer,\n",
    "                            'num_layers': num_layers,\n",
    "                            'bias': bias,\n",
    "                            'batch_first': True,\n",
    "                            'dropout': dropout,\n",
    "                            'bidirectional': bidirectional}\n",
    "        \n",
    "        # Set our FC layer parameters\n",
    "        self.linear_config = {'in_features': hidden_layer,\n",
    "                              'out_features': vocab_size,\n",
    "                              'bias': bias}\n",
    "        \n",
    "        # Instanciate the layers\n",
    "        self.encoder = nn.LSTM(**self.lstm_config)\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('linear', nn.Linear(**self.linear_config))\n",
    "        self.decoder.add_module('softmax',nn.LogSoftmax(dim=-1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs, _ = self.encoder(inputs)\n",
    "        predictions = self.decoder(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(len(chars), len(chars), 128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Transformación del input\n",
    "\n",
    "Una vez que definimos la arquitectura de la red, sabemos con exactitud cuál es el input que necesitamos utilizar. En esta sección transformaremos el texto que leimos del archivo en ejemplos de entrenamiento para nuestra red. El resultado será una matrix que representa las secuencias de caracteres y una matriz que representa las etiquetas correspondientes.\n",
    "\n",
    "  - ¿Cómo debemos representar cada ejemplo?\n",
    "  - ¿Cómo debemos representar cada etiqueta?\n",
    "  \n",
    "Usaremos la clase torch.Dataset para implementar nuestro. Necesitamos implementar dos métodos para esto:\n",
    "  - \\__len__: retorna el largo del dataset\n",
    "  - \\__getitem__: dado un id, retorna el elemento asociado en el dataset con ese id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MartinFierroDataset(Dataset):\n",
    "    def __init__(self, textdata, maxlen):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for i in range(0, len(textdata) - maxlen - 1, maxlen):\n",
    "            sentences.append(textdata[i: i + maxlen])\n",
    "            next_chars.append(textdata[i + 1: i + maxlen + 1])\n",
    "        self.length = len(sentences)\n",
    "\n",
    "        self.X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "        self.y = np.zeros((len(sentences), maxlen), dtype=np.float32)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                self.X[i, t, char_indices[char]] = 1\n",
    "                self.y[i, t] = char_indices[next_chars[i][t]]\n",
    "        \n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        output = {'X': self.X[idx],\n",
    "                  'y': self.y[idx]}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MartinFierroDataset(text, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenamiento de la red\n",
    "\n",
    "En esta sección entrenaremos nuestra red. Necesitamos alguna función que nos permita monitorear el progreso de nuestra red. Para eso vamos a imprimir una muestra del texto generado por la red cada cierta cantidad de epochs.\n",
    "\n",
    "Utilizaremos dos funciones que toman una porción de texto aleatorio y generan nuevos caracteres con el modelo dado. \n",
    "\n",
    "    - ¿Cómo podemos interpretar la salida de la red? ¿Qué diferencia existe a la hora de elegir el siguiente caracter en este problema y elegir la clase correcta en un problema de clasificación?\n",
    "    - ¿Qué hacen estas funciones? ¿Para qué se utiliza la variable diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def temperature_sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\\n\"\n",
    "    temp_preds = np.asarray(preds[:,-1,:]).astype('float64') / temperature\n",
    "    exp_preds = np.exp(temp_preds)\n",
    "    new_probs = (exp_preds / np.sum(exp_preds)).squeeze()\n",
    "    probas = np.random.multinomial(1, new_probs, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def print_sample(model, device, maxlen=50):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_size = 200\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            #sentence = 'el bien perdido'\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(sentence)\n",
    "\n",
    "            # Printing the sample\n",
    "            for i in range(sample_size):\n",
    "                x = np.zeros((1, maxlen, len(chars)), dtype=np.float32)\n",
    "                # Build the one-hot encoding for the sentence\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                input_tensor = torch.tensor(x).to(device)\n",
    "                    \n",
    "                logprob_preds = model(input_tensor)\n",
    "                next_index = temperature_sample(logprob_preds.cpu().numpy(), diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Primero configuramos los hiperparámetros de la red. En este momento determinamos lo siguiente:\n",
    "  -  learning_rate\n",
    "  -  epochs\n",
    "  -  función de pérdida\n",
    "  -  optimizador\n",
    "  \n",
    "También definimos los parámetros para torch.DataLoader, clase que implementa un manejador del dataset que nos dividirá los datos en batches (y los distribuirá entre distintos nodos de cómputo, en caso de contar con multi GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 22\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "dataloader_config = {'dataset': data,\n",
    "                     'batch_size': 32,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "# Send the model to GPU if there is one available\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "historical_loss = torch.FloatTensor()\n",
    "\n",
    "# Set the model on train mode\n",
    "model.train()\n",
    "for epoch in range(1,epochs+1):\n",
    "    loss = 0\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # Show samples every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print_sample(model, device)\n",
    "        model.train()\n",
    "\n",
    "    train_loss = torch.FloatTensor().to(device)\n",
    "        \n",
    "    dataloader = DataLoader(**dataloader_config)\n",
    "    for i_batch, sample in enumerate(dataloader):\n",
    "        inputs, gt_out = sample['X'].to(device), sample['y'].to(device)\n",
    "        preds = model(inputs)\n",
    "        bs, seqlen, cat = preds.size()\n",
    "\n",
    "        # preds: batch_size x max_seq_length x len(chars)\n",
    "        # gt_out: batch_size x max_seq_length\n",
    "        # NLLLoss expects inputs of the form:\n",
    "        # N x C x d1 x ... x dt for the input\n",
    "        # N x d1 x ... x dt for the targets\n",
    "        # So we transform define N = batch_size x max_seq_length\n",
    "        # and we keep C = len(chars)\n",
    "        loss = loss_function(preds.view(bs*seqlen, -1),\n",
    "                             gt_out.view(bs*seqlen).type(torch.long)).unsqueeze(0)\n",
    "        \n",
    "        # Set gradients to 0, backpropagate, make an uptimization\n",
    "        # update and store the loss for logging purposes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = torch.cat([train_loss, loss])      \n",
    "        \n",
    "    print(\"Epoch %03d, Time taken %.2f, Training-Loss %.5f\" % (epoch, time()-start, torch.mean(train_loss)))\n",
    "    with torch.no_grad():\n",
    "        historical_loss = torch.cat([historical_loss, torch.mean(train_loss.cpu()).view(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ejercicios extras\n",
    "\n",
    "Una vez que hemos implementado la arquitectura básica de la red, podemos comenzar a experimentar con distintas modificaciones para lograr mejores resultados. Algunas tareas posibles son:\n",
    "\n",
    " - Agregar más capas recurrentes\n",
    " - Probar otros largos de secuencias máximas\n",
    " - Agregar capas de regularización y/o dropout\n",
    " - Agregar métricas de performance como perplexity y word error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones\n",
    "\n",
    "Para asegurarnos de que el modelo esté efectivamente entrenando, podemos graficar la función de pérdida en el corpus de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "loss_values = historical_loss.detach().numpy()\n",
    "seaborn.lineplot(x=range(loss_values.shape[0]), y=loss_values)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
