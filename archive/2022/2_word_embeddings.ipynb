{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2234f974",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Representación de palabras mediante vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d828c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Créditos\n",
    "\n",
    "Este material está fuertemente inspirado en los cursos de Stanford [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/) y [CS224u: Natural Language Understanding](http://web.stanford.edu/class/cs224u/), así también como del [blog de Jay Alammar](http://jalammar.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3f6a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conceptos básicos de representación de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a91e64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Características de las palabras\n",
    "\n",
    "- Suelen ser **unidad básica** en cualquier tarea de procesamiento de lenguaje natural (PLN).\n",
    "    - Hoy en día, cuando hablamos de \"palabras\" podemos referirnos a otros símbolos como subpalabras o incluso conjuntos de caracteres.\n",
    "- Gran parte del trabajo de PLN solía representar las palabras como **símbolos atómicos**.\n",
    "    - Esto ya no es así salvo algunas excepciones.\n",
    "- Cualquier modelo de PLN requiere como entrada la **representación de una palabra**.\n",
    "- Las nociones de **similitud y distancia** entre palabras son cruciales para tareas de PLN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44340e04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representación discreta\n",
    "\n",
    "- Para aplicar medidas de similitud y distancia (e.g. euclídea, coseno), utilizamos **vectores que representen palabras**.\n",
    "- Primera aproximación: **vectores *one-hot***.\n",
    "- Para cada palabra del vocabulario, tenemos un vector.\n",
    "- En términos de espacio vectorial, el vector tiene un **1** en una de sus dimensiones y __0s__ en todas las demás.\n",
    "- **Dimensionalidad muy alta** (el inglés tiene un estimado de 13 millones de palabras).\n",
    "- Los vectores son **ralos** (o esparsos), tienen muchos ceros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62b364",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "Dado un corpus:\n",
    "\n",
    "- \"El fallo fue decisivo\"\n",
    "- \"La corte rectificará el fallo\"\n",
    "\n",
    "Tenemos un vocabulario $V = \\{corte, decisivo, el, fallo, fue, la, rectificará\\}$.\n",
    "\n",
    "Codificamos un número $|V| = 7$ de vectores para cada palabra $w^{(i)} \\in \\mathbb{R}^{|V|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce04ad1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo de representación discreta\n",
    "\n",
    "$$\n",
    "    w^{corte} =\n",
    "    \\begin{bmatrix}\n",
    "       1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{decisivo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{el} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{fallo} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    w^{fue} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\n",
    "    \\end{bmatrix}\n",
    "    w^{la} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "    \\end{bmatrix} ,\n",
    "    w^{rectificará} =\n",
    "    \\begin{bmatrix}\n",
    "       0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001811a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"El fallo fue decisivo\", \n",
    "    \"La corte rectificará el fallo\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [\n",
    "    [word.lower() for word in doc.split()] for doc in corpus\n",
    "]\n",
    "\n",
    "vocabulary = sorted(set([\n",
    "    word for doc in tokenized_corpus for word in doc\n",
    "]))\n",
    "\n",
    "one_hot_vectors = np.eye(len(vocabulary), dtype=np.int32)\n",
    "\n",
    "for doc in tokenized_corpus:\n",
    "    print(np.array([\n",
    "        one_hot_vectors[vocabulary.index(word), :] for word in doc\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a3c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación distribuída de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d759fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Concepto\n",
    "\n",
    "- Idea: Representar una palabra **mediante sus vecinos**.\n",
    "    - Es una de las **ideas más importantes** en PLN moderno.\n",
    "\n",
    "#### Ejemplo: \n",
    "\n",
    "- \"_El gobierno rescató a los_ **bancos** _ante la crisis de crédito_\".\n",
    "- \"_La municipalidad reparará los_ **bancos** _de la plaza_\".\n",
    "\n",
    "En estos ejemplos, las palabras que están en _cursiva_ representarán las palabras en **negrita**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96d92c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivación\n",
    "\n",
    "- La representación distribuída nos da pie a pensar como los vectores **codifican el significado de las unidades linguísticas** (palabras, oraciones, documentos, etc.).\n",
    "- Son la base de los **modelos de espacio vectorial**.\n",
    "- Piezas fundamentales en modelos para **Procesamiento y Comprensión de Lenguaje Natural**.\n",
    "- Estas representaciones pueden ser usadas, entre otros, para:\n",
    "    - Entender y modelar fenómenos sociales y lingüísticos.\n",
    "    - Entrada para modelos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87573a56",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representación mediante matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ecea3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos del diseño de matrices\n",
    "\n",
    "- Buscamos representar palabras de manera matemática.\n",
    "    - Idea: Mediante una matriz (vista como un conjunto de vectores)\n",
    "- Hay que definir la construcción de una matriz.\n",
    "    - Muchas opciones de diseño.\n",
    "    - Distinto impacto tendrán las decisiones tomadas respecto al modelado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b2d0a",
   "metadata": {},
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "- Cada palabra se representa por las palabras a su alrededor (dada una ventana).\n",
    "- La matriz es simétrica (independiente de la posición de las palabras).\n",
    "    - Esto se conoce como \"bolsa de palabras\" (o _bag of words_)\n",
    "- Dado un vocabulario $V$:\n",
    "    - Matriz de co-ocurrencias $X \\in \\mathbb{R}^{|V|\\times|V|}$\n",
    "    - $X_{ij}$ representa la co-ocurrencia entre la palabras $w^{(i)}$ y $w^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892e0bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de co-ocurrencia de palabras\n",
    "\n",
    "<img src=\"images/word-word-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365049e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "- Cada documento se representa por las palabras que lo conforman.\n",
    "- Se usa un modelo de bolsa de palabras: no importa la posición.\n",
    "- Dadas $N$ palabras en un corpus de $M$ documentos:\n",
    "    - Matriz de documentos $X \\in \\mathbb{R}^{N \\times M}$\n",
    "    - $X_{ij}$ representa la ocurrencia de la palabra $w^{(i)}$ en el documento $d^{(j)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe62e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matriz de documentos\n",
    "\n",
    "<img src=\"images/term-document-matrix.png\" width=\"90%\"/>\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d720d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- La ventana define la **cantidad de palabras en el contexto** a considerar.\n",
    "- La ponderación (*scaling*) le da un **peso a la palabra** en la suma total.\n",
    "    - Igual peso para todas las palabras, i.e. bolsa de palabras.\n",
    "    - Peso distinto de acuerdo a la distancia, e.g. decaimiento fraccional.\n",
    "    - Peso binario, i.e. la palabra está o no.\n",
    "\n",
    "<img src=\"images/window-scaling.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"http://web.stanford.edu/class/cs224u/\" target=\"_blank\">http://web.stanford.edu/class/cs224u/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e886b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ventana y ponderación\n",
    "\n",
    "- Ventanas más grandes y con peso igual (_flat_) capturan **información semántica**.\n",
    "    - Las matrices de documentos dan idea de la temática del documento.\n",
    "- Ventanas más cortas y con pesos capturan **información sintáctica** (colocacional).\n",
    "    - Las matrices de co-ocurrencias de palabras son de afinidad de palabras.\n",
    "- El límite puede definirse más allá de una ventana fija.\n",
    "    - Utilizar oraciones, párrafos o documentos tendrá consecuencias distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6f1d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Consideraciones\n",
    "\n",
    "- Las matrices **incrementan con el vocabulario**.\n",
    "    - Definir límites en vocabulario o bien lidiar con modelos muy grandes.\n",
    "    - Por la ley de Zipf (Zipf, 1949), la lista de palabras con escasa aparición se vuelve muy grande.\n",
    "- La elección de la ventana afecta en la obtención de **matrices más o menos dispersas**.\n",
    "- Representar distintas unidades lingüísticas deriva en **más dispersión** (ver el ejemplo de word-word vs term-document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ed41f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01aa3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objetivos de reducir dimensiones\n",
    "\n",
    "- Buscamos asociaciones de alto orden entre los datos.\n",
    "    - Estas no siempre están presentes en los conteos o los esquemas de reponderación.\n",
    "- Se busca capturar nociones de co-ocurrencia de mayor orden.\n",
    "- Podemos considerar dos palabras: \"bizcocho\" y \"criollito\". \n",
    "    - En muchos esquemas pueden referirse a lo mismo.\n",
    "    - \"Bizcocho\" es más común en Buenos Aires, mientras que \"criollito\" es de Córdoba.\n",
    "    - Pueden que en un mismo texto no aparezca nunca juntas.\n",
    "    - Queremos capturar la correlación entre ambas por más que la co-ocurrencia sea 0.\n",
    "- Las técnicas de reducción de dimensionalidad suelen lograr encontrar similaridades semánticas en segundo plano.\n",
    "- Por otro lado reducen el tamaño de los modelos y los hacen menos dispersos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6368b816",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Análisis de Semántica Latente (LSA)\n",
    "\n",
    "- Es uno de los modelos de reducción de dimensionalidad que más se ha utilizado (Deerwester et al., 1990).\n",
    "- Consiste en aplicar **descomposición de valores singulares truncada** (_truncated SVD_), de álgebra lineal, sobre los VSMs.\n",
    "- Establece un baseline bastante estándar y muchas veces difícil de superar.\n",
    "- Con un vocabulario muy grande tiene problemas para escalar pues tiene costo cuadrático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710b189",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de LSA\n",
    "\n",
    "- Generar la matriz del VSM $X$\n",
    "- Aplicar SVD sobre $X$ de manera que $X = USW^{T}$.\n",
    "- $U$ y $W$ son matrices ortonormales: Sus columnas están normalizadas por longitud y son ortogonales entre si (la distancia coseno es 1).\n",
    "- $S$ es una matriz diagonal con valores singulares ordenados por tamaño (la primera dimensión corresponde a la fuente de mayor variabilidad en los datos, y así sucesivamente).\n",
    "- normalizadas en longitud, y ortogonales la una de la otra\n",
    "- Observando los _valores singulares_ los truncamos en un índice $k$.\n",
    "- Tomamos $\\hat{X} = U_{1:|V|,1:k}S_{1:k,1:k}$, donde $|V|$ es el tamaño de nuestro vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3a8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algoritmo de LSA\n",
    "\n",
    "<img src=\"images/lsa.png\" width=\"90%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1c67b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problemas de LSA\n",
    "\n",
    "- Las palabras \"vacías\" (artículos, pronombres, conectores, etc.) son muy frecuentes.\n",
    "    - Se puede ignorar o limitar.\n",
    "    - Se puede reponderar las palabras mediante técnicas como [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) o [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "- La dimensión de la matriz puede cambiar a medida que hay nuevas palabras, cambiando el temaño del vocuabulario $|V|$.\n",
    "- Las matrices de co-ocurrencia suelen ser muy ralas.\n",
    "- El costo de SVD es cuadrático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed562e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1c241",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vectores densos de palabras\n",
    "\n",
    "- La idea es directamente **aprender vectores densos**.\n",
    "- Se crea un modelo que puede ser **entrenado de manera iterativa**.\n",
    "- El modelo codifica la **probabilidad de una palabra dado su contexto**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b9341",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Predecir la siguiente palabra\n",
    "\n",
    "<img src=\"images/next-word.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb50da4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modelos de lenguaje\n",
    "\n",
    "- Dada una secuencia de palabras de entrada, el modelo se entrena para predecir la palabra que continúa la oración.\n",
    "\n",
    "<img src=\"images/language-model.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f9a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embedding Lookup\n",
    "\n",
    "- Los embeddings se generan en una **lookup matrix**.\n",
    "- Las dimensiones de la matriz representan **rasgos latentes** de las palabras.\n",
    "- Los rasgos son pesos, se optimizan para mejorar la predicción del modelo.\n",
    "\n",
    "<img src=\"images/language-model-embedding.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539d432",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generación de datos: Autosupervisados\n",
    "\n",
    "- El **texto libre** se puede utilizar para generar datos.\n",
    "- Son datos no supervisados transformados a una tarea supervisada: Esto se conoce como **autosupervisión o self-supervision**.\n",
    "- A partir de una **ventana** generamos nuestros datos de entrada y la etiqueta (palabra) de salida.\n",
    "\n",
    "<img src=\"images/lm-dataset.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4229b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b414364",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Un algoritmo para calcular embeddings\n",
    "\n",
    "- Surge del **trabajo de Mikolov (2013)**.\n",
    "- La idea principal es **predecir una palabra dado su contexto**.\n",
    "- Se presentaron dos maneras de realizarlo:\n",
    "    - CBOW (Continous Bag-of-Words): Dado un contexto **prededir la palabra central**.\n",
    "    - Skipgram: Dada la palabra centrar **predecir el contexto**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827090b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Continous Bag-of-Words\n",
    "\n",
    "<img src=\"images/cbow-1.png\" width=\"90%\">\n",
    "<img src=\"images/cbow-2.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db9835",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skipgram\n",
    "\n",
    "<img src=\"images/skipgram.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0415d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "- Generamos el conjunto de datos.\n",
    "- El modelo arranca con pesos aleatorios.\n",
    "- Se entrenará con sucesivas iteraciones.\n",
    "\n",
    "<img src=\"images/word2vec-1.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecba056",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "<img src=\"images/word2vec-2.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968ba80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative Sampling\n",
    "\n",
    "- El hecho de tener que proyectar a todo el vocuabulario hace que el entrenamiento se vuelva caro.\n",
    "\n",
    "<img src=\"images/neg-sam-1.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b8414",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative Sampling: Cambio de tarea\n",
    "\n",
    "<img src=\"images/neg-sam-2.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6cc4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative Sampling: Nueva tarea\n",
    "\n",
    "<img src=\"images/neg-sam-3.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29ab4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative Sampling: Cambio de labels\n",
    "\n",
    "<img src=\"images/neg-sam-4.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004463e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Negative Sampling: Agregar ejemplos negativos\n",
    "\n",
    "<img src=\"images/neg-sam-5.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db84d51",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Matrices\n",
    "\n",
    "<img src=\"images/word2vec-3.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700d1ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Viendo un batch\n",
    "\n",
    "<img src=\"images/word2vec-4.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806cf4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Obteniendo los embeddings\n",
    "\n",
    "<img src=\"images/word2vec-5.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0752e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Estimando el error\n",
    "\n",
    "<img src=\"images/word2vec-6.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba5eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Vectores finales\n",
    "\n",
    "- Finalizamos con dos vectores por cada palabra: los de input y los de output.\n",
    "    - Estos representan la palabra central y/o el contexto de acuerdo al modelo (CBOW vs. Skipgram).\n",
    "- Ambos vectores tienen información de coocurrencia.\n",
    "- ¿Qué hacemos para obtener un vector final?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3eb3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2vec: Algunas propiedades\n",
    "\n",
    "- Las dimensiones de los embeddings son latentes. \n",
    "    - Codifican información, pero son difíciles de interpretar.\n",
    "- Los vectores suelend codificar dimensiones de similitud.\n",
    "- Aplicando operaciones aritméticas obtenemos valores interesantes en los resultados.\n",
    "- Son entrenados de manera supervisada y pueden servir como input a otros modelos de aprendizaje supervisado.\n",
    "\n",
    "<img src=\"images/king-analogy.png\" width=\"90%\">\n",
    "<div style=\"text-align: right; font-size:.9em;\">Source: <a href=\"https://jalammar.github.io/illustrated-word2vec/\" target=\"_blank\">https://jalammar.github.io/illustrated-word2vec/</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305ed16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word2Vec: Ejemplo via Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2d741",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!gunzip < ./data/glove.6B.50d.txt.gz > ./data/glove.6B.50d.txt\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"./data/glove.6B.50d.txt\", binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3fc434",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(model[\"king\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d781bb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(model[\"king\"] - model[\"man\"] + model[\"woman\"], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447dcec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model.most_similar(model[\"boy\"] - model[\"man\"] + model[\"woman\"], topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ecad8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American society for information science, 41(6), 391-407.\n",
    "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "- Zipf, G. K. (1949). Human behavior and the principle of least effort."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
