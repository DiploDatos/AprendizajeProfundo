{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b02df97-cbb3-4759-9371-cbbecd0ccd86"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construyendo una red neuronal con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "03d05899-7ff9-4413-aa67-c7a96bbdfcde"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué librerías necesitamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fa44eec5-93b3-4e4b-adcb-1065bb5cc474"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otras librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "79849185-892a-41ce-b4a2-26db6ad19597"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cargando los datos del MNIST\n",
    "\n",
    "- El conjunto de datos a utilizar es el **[MNIST](http://yann.lecun.com/exdb/mnist/)**.\n",
    "- Es un conjunto estándar para hacer *reconocimiento de imágenes*.\n",
    "- Buscamos entrenar un clasificador que reconozca que dígito es mostrado en la imagen.\n",
    "- El MNIST está compuesto por imágenes de 28x28 píxeles representadas como matrices.\n",
    "- La salida son 10 clases (dígitos del 0 al 9).\n",
    "- Se preprocesará para *convertir las matrices en vectores* y transformar las etiquetas en representaciones *one-of-k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # For mini-batch gradient descent\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "input_size = 28*28\n",
    "train_examples = 60000\n",
    "test_examples = 10000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(train_examples, input_size)\n",
    "x_test = x_test.reshape(test_examples, input_size)\n",
    "\n",
    "# normalize the input\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contruyendo la red neuronal\n",
    "\n",
    "- Comenzaremos por construir un *perceptrón multicapa* que es la red neuronal más común.\n",
    "- El modelo más simple en Keras es una concatenación de capas (layers).\n",
    "    - Se llama modelo secuencial.\n",
    "- La capa más básica es la *densa* (dense o fully connected).\n",
    "    - Internamente tiene dos variables: una matriz de pesos y un vector de biases. Keras nos abstrae de todo eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input to hidden layer\n",
    "model.add(Dense(512, input_shape=(input_size,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Hidden to output layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para imprimir una descripción del modelo existe un comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funciones de Activación\n",
    "\n",
    "- Una red neuronal con activación lineal no tiene mucho más poder de representación que un algoritmo lineal.\n",
    "- Para expresar no linearidad en la red neuronal se necesitan funciones no lineales de activación.\n",
    "- Una función de activación común es la *sigmoide* (o logística).\n",
    "- Keras soporta varias funciones de activación: rectified linea unit (ReLU), tangenge hiperbólica, sigmoide \"dura\", etc.\n",
    "    - Hoy en día, por sus propiedades, ReLU suele ser la más utilizada [1].\n",
    "- La función de activación *softmax* es utilizada al final de la red y sirve para clasificación.\n",
    "\n",
    "![Funciones de activación](images/activation_functions.png \"Funciones de activación\")\n",
    "<div style=\"text-align: right;\">Fuente: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 52,650\n",
      "Trainable params: 52,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularización de la red\n",
    "\n",
    "### Regularización de los pesos\n",
    "\n",
    "- La red puede regularizarse penalizando los pesos.\n",
    "- Los pesos se regularizan mediante alguna norma:\n",
    "    - L1 es la suma del valor absoluto: ${\\displaystyle \\lambda \\sum_{i=1}^{k} |w_i|}$\n",
    "    - L2 es la suma del valor cuadrado, es la más común: ${\\displaystyle \\lambda \\sum_{i=1}^{k} w_i^2}$\n",
    "    - Elastic net es una combinación de ambas: ${\\displaystyle \\lambda_1 \\sum_{i=1}^{k} |w_i| + \\lambda_2 \\sum_{i=1}^{k} w_i^2}$\n",
    "- Para un análisis detallado de la diferencia entre L1 y L2 revisar [\\[2\\]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout\n",
    "\n",
    "- Otra forma muy usada a la hora de regularizar es el **dropout** [3].\n",
    "- Es extremadamente efectivo y simple.\n",
    "- Es complementario a L1/L2/ElasticNet.\n",
    "- Durante el entrenamiento se implementa apagando un neurón con alguna probabilidad **_p_** (un hiperparámetro).\n",
    "\n",
    "![Dropout](images/dropout.jpeg \"Dropout\")\n",
    "<div style=\"text-align: right;\">Fuente: Trabajo de Srivastava et al. [3]</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout en Keras\n",
    "\n",
    "- Se aplica agregando capas al modelo.\n",
    "- Se llaman capas `Dropout` y se agrega a cada capa que se quiere regularizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- En general, para acelerar la convergencia de la red, se normalizan los features de entrada, de manera que todos estén en un rango similar.\n",
    "- Esta idea también puede llevarse a las capas ocultas de la red.\n",
    "- La idea de la \"Normalización por Lotes\" (*Batch Normalization*) [4] es reducir el rango en el que se mueven los valores de las neuronas ocultas.\n",
    "- La manera en que se hace esto es restarle, a cada salida de cada capa oculta, la media del lote (batch) de datos de entrenamiento y dividirlo por la desviación estándar (a grandes razgos).\n",
    "- Como resultado, la red converge más rápido e incluso se genera un efecto de regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### BatchNormalization en Keras.\n",
    "\n",
    "- Se aplica agregando capas al modelo.\n",
    "- Se llaman capas `BatchNormalization` y se agrega a cada capa que se quiere normalizar.\n",
    "- El `momentum` es un parámetro que decide cuánta información de los lotes anteriores se tiene en cuenta a la hora de normalizar el lote actual (en el trabajo original, este es de `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    BatchNormalization(momentum=0),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparando el modelo para entrenarlo\n",
    "\n",
    "- Para minimizar una red neuronal necesitamos *calcular sus gradientes*.\n",
    "    - Esto se hace con el algoritmo de *retropropagación*.\n",
    "- Keras tiene la capacidad de hacerlo automáticamente.\n",
    "    - Esto se conoce como _diferenciación automática_ y es algo común en los frameworks de deep learning.\n",
    "- El modelo de Keras necesita *compilarse*.\n",
    "    - Recordar que lo que armamos en Keras (o TensorFlow) es un grafo de computación.\n",
    "- Al compilar el modelo los parámetros más importantes son la función de costo y el algoritmo de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funciones de costo y algoritmos de optimización\n",
    "\n",
    "- La función de costo puede cambiar de acuerdo al tipo de problema (clasificación binaria/multiclase o regresión).\n",
    "    - La funciones más comunes son la media del error cuadrático (_mean squared error_) para regresión y la entropía cruzada (_crossentropy_) para clasificación.\n",
    "- El algoritmo de optimización es el que entrena la red. Existen varios, que en si son variaciones del algoritmo de _descenso por la gradiente_.\n",
    "\n",
    "<div style=\"text-align: center; margin: 5px 0;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/contours_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/saddle_point_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"text-align: right;\">Fuente: http://ruder.io/optimizing-gradient-descent/</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compilando el modelo Keras y visualizando la arquitectura\n",
    "\n",
    "- Con el método `.compile()` podemos compilar el modelo de Keras.\n",
    "- Además de la función de costo y el algoritmo de optimización se le pueden pasar métricas para llevar registro además del error de los datos (e.g. la exactitud o la precisión).\n",
    "- Una vez compilado el modelo la modificación requerirá rehacerlo desde cero (salvo que se guarden y carguen los pesos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                  # También podría ser el string \"Adagrad\" con los parámetros por defecto\n",
    "              metrics=['accuracy'])  # La métrica sirve para llevar algún registro además del costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizar la arquitectura (opcional)\n",
    "\n",
    "Opcionalmente, si instalamos las librerías extras pedidas en el setup y utilizando `vis_util`, podemos visualizar el grafo de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 254.00 337.00\" width=\"254pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 250,-333 250,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139950494493552 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139950494493552</title>\n",
       "<polygon fill=\"none\" points=\"38,-292.5 38,-328.5 208,-328.5 208,-292.5 38,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-306.8\">dense_10_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139950493839144 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139950493839144</title>\n",
       "<polygon fill=\"none\" points=\"68.5,-219.5 68.5,-255.5 177.5,-255.5 177.5,-219.5 68.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-233.8\">dense_10: Dense</text>\n",
       "</g>\n",
       "<!-- 139950494493552&#45;&gt;139950493839144 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139950494493552-&gt;139950493839144</title>\n",
       "<path d=\"M123,-292.4551C123,-284.3828 123,-274.6764 123,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"126.5001,-265.5903 123,-255.5904 119.5001,-265.5904 126.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139950494491368 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139950494491368</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 246,-182.5 246,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-160.8\">batch_normalization: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139950493839144&#45;&gt;139950494491368 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139950493839144-&gt;139950494491368</title>\n",
       "<path d=\"M123,-219.4551C123,-211.3828 123,-201.6764 123,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"126.5001,-192.5903 123,-182.5904 119.5001,-192.5904 126.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139950494492320 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139950494492320</title>\n",
       "<polygon fill=\"none\" points=\"60.5,-73.5 60.5,-109.5 185.5,-109.5 185.5,-73.5 60.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 139950494491368&#45;&gt;139950494492320 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139950494491368-&gt;139950494492320</title>\n",
       "<path d=\"M123,-146.4551C123,-138.3828 123,-128.6764 123,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"126.5001,-119.5903 123,-109.5904 119.5001,-119.5904 126.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139950494492544 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139950494492544</title>\n",
       "<polygon fill=\"none\" points=\"68.5,-.5 68.5,-36.5 177.5,-36.5 177.5,-.5 68.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"123\" y=\"-14.8\">dense_11: Dense</text>\n",
       "</g>\n",
       "<!-- 139950494492320&#45;&gt;139950494492544 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139950494492320-&gt;139950494492544</title>\n",
       "<path d=\"M123,-73.4551C123,-65.3828 123,-55.6764 123,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"126.5001,-46.5903 123,-36.5904 119.5001,-46.5904 126.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, dpi=72).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "- Una vez compilado el modelo, está listo para ser entrenado.\n",
    "- Keras tiene una interfaz similar a Scikit-Learn, con lo método `fit` y `predict`.\n",
    "- Para entrenar se necesitan 3 parámetros:\n",
    "    - El conjunto de datos de entrenamiento (datos y etiquetas).\n",
    "    - El tamaño del batch para hacer _mini-batch gradient descent_.\n",
    "    - La cantidad de épocas que entrenar.\n",
    "    - Eventualmente le podemos pasar datos para hacer validación del modelo.\n",
    "    - El parámetro `verbose` nos imprime información útil respecto al desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 2.3263 - accuracy: 0.5613 - val_loss: 1.3093 - val_accuracy: 0.8419\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 1.3102 - accuracy: 0.7959 - val_loss: 0.9619 - val_accuracy: 0.8877\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 1.0354 - accuracy: 0.8401 - val_loss: 0.7856 - val_accuracy: 0.9063\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.8692 - accuracy: 0.8624 - val_loss: 0.6566 - val_accuracy: 0.9166\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.7500 - accuracy: 0.8788 - val_loss: 0.5791 - val_accuracy: 0.9218\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.6584 - accuracy: 0.8895 - val_loss: 0.5163 - val_accuracy: 0.9231\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 42us/sample - loss: 0.5907 - accuracy: 0.8953 - val_loss: 0.4348 - val_accuracy: 0.9356\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.5342 - accuracy: 0.9026 - val_loss: 0.3956 - val_accuracy: 0.9385\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.4907 - accuracy: 0.9069 - val_loss: 0.3612 - val_accuracy: 0.9411\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.4548 - accuracy: 0.9115 - val_loss: 0.3309 - val_accuracy: 0.9424\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(x_test, y_test), verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- [1] LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444.\n",
    "- [2] \"Differences between L1 and L2 as Loss Function and Regularization\". http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "- [3] Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of machine learning research 15, no. 1 (2014): 1929-1958. Harvard.\n",
    "- [4] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
