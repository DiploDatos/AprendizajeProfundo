{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DiploDatos/AprendizajeProfundo/blob/master/5_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WAMkzi2jF5F"
   },
   "source": [
    "# Construyendo una red convolucional con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpGbtqhyjF5H"
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2SgMX7x4jF5H",
    "outputId": "be8f50da-4062-4c22-9e7e-5407ac3d193e"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import mlflow\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tempfile\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, average_precision_score,confusion_matrix\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndqTFN4-jF5J"
   },
   "source": [
    "## Red convolucional para imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atOCl7WojF5J"
   },
   "source": [
    "### Datos del CIFAR-10\n",
    "\n",
    "Utilizamos los mismos datos que se usaron en el [notebook 1](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/1_basic_mlp.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w_3ScPbsjF5K",
    "outputId": "5bc56f47-1047-4e68-f8ab-33fb6f079c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "transform = transforms.Compose( #Agrupa todas las transformaciones que le pedimos\n",
    "    [transforms.ToTensor(),      # en este caso, convertimos a tensores y normalizamos los datos\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "## Train\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                        train=True,\n",
    "                                        download=True, \n",
    "                                        transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2)\n",
    "## Test\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                       train=False,\n",
    "                                       download=True, \n",
    "                                       transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, \n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ryj92zWjF5K"
   },
   "source": [
    "### Red convolucional\n",
    "\n",
    "- La red convolucional se obtiene apilando capas [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html). \n",
    "    - En particular, este tipo de capas acepta matrices (a diferencia de la lineal que sólo acepta vectores). En las capas se definen lo canales de entrada y los de salida, además del tamaño del kernel (i.e. ventana). \n",
    "- También son comunes las capas [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) que realizan una operación de max pooling, en 2 dimensiones. \n",
    "- La red se completa con algunas capas lineales [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear) para poder llevarla a las 10 dimensiones de salida que vienen a representar las clases.\n",
    "\n",
    "**Funciones auxiliares**:\n",
    "-  [`Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html?highlight=view#torch.Tensor.view): Es similar a [`reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) de numpy, cambia las dimensiones de un tensor sin guardarlo en memoria y sin cambiar los valores del input original. El parámetro `-1` se usa cuando no sabemos cuántas filas queremos pero conocemos la cantidad de columnas (esto se extrapola a más dimensiones).\n",
    "- [`Optimizer.zero_grad`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#:~:text=Sets%20the%20gradients%20of%20all,set%20the%20grads%20to%20None.): En cada paso de entrenamiento necesitamos resetear los valores del gradiente para que no se cuenten \"2 veces\". Para mayor detalle visitar este [link](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch#:~:text=384,of%20maximization%20objectives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bFm2gEsEjF5L",
    "outputId": "941c5010-a8f2-4148-ab88-da1a03bfc166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # (#canales de entrada, #canales de salida, #filtros)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # (kernel_size , stride)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # (#canales de entrada, #canales de salida, #filtros)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # (input, output)\n",
    "        self.fc2 = nn.Linear(120, 84) # (input, output)\n",
    "        self.fc3 = nn.Linear(84, 10) # (input, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) #Aplicamos relu a la primera capa convolucional, luego pooling\n",
    "        x = self.pool(F.relu(self.conv2(x))) #Aplicamos relu a la segunda capa convolucional, luego pooling\n",
    "        x = x.view(-1, 16 * 5 * 5) # Necesitamos transformarlo en un vector para que sea input de la capa lineal\n",
    "        x = F.relu(self.fc1(x)) # Finalmente relu a cada capa convolucional\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glc3AeCejF5M"
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "La red se entrena igual que el caso del perceptrón multicapa, solo que esta vez no requiere reacomodar la matriz de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "cbaf6c321a6c4339b67bdda8db3f63c4",
      "5137f24eb96c4c23b135a48d4bc1735e",
      "897566bef8af4988aac9f61607a1a762"
     ]
    },
    "id": "gHxSqhQtjF5M",
    "outputId": "040c903c-17aa-4f83-8ba5-ff5cb3d15a47"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd833eca2ef47b0b90c506ebdaf9560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540cbedb378042dba4befba617676008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train loss: NaN:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5622fcb019e4249a3af5ffac4ad138c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train loss: NaN:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "model.train()\n",
    "iters_per_epoch = len(trainloader)\n",
    "for epoch in trange(EPOCHS):  # loop over the dataset multiple times\n",
    "    pbar = tqdm(trainloader, desc=\"Train loss: NaN\")\n",
    "    for data in pbar:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Train loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ67KAC0jF5M"
   },
   "source": [
    "### Evaluación\n",
    "\n",
    "Una vez más, la evaluación es similar al caso del perceptrón multicapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ed8dfa44b5614920add216ff01336686"
     ]
    },
    "id": "NeLZrbsWjF5N",
    "outputId": "6bcbe299-6939-4063-b1ae-449b2b31bb1d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86c4912e4a54d65a23c1d2a746c3c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       plane       0.53      0.62      0.57      1000\n",
      "         car       0.66      0.61      0.64      1000\n",
      "        bird       0.43      0.42      0.42      1000\n",
      "         cat       0.35      0.40      0.37      1000\n",
      "        deer       0.48      0.25      0.33      1000\n",
      "         dog       0.50      0.35      0.42      1000\n",
      "        frog       0.60      0.55      0.57      1000\n",
      "       horse       0.53      0.64      0.58      1000\n",
      "        ship       0.48      0.72      0.57      1000\n",
      "       truck       0.57      0.52      0.54      1000\n",
      "\n",
      "    accuracy                           0.51     10000\n",
      "   macro avg       0.51      0.51      0.50     10000\n",
      "weighted avg       0.51      0.51      0.50     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(testloader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=CIFAR_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xkSW-PYjjF5N"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame((confusion_matrix(y_true, y_pred)),columns=list(CIFAR_CLASSES),index=list(CIFAR_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bk_Ru6xGjF5N",
    "outputId": "d11ff701-af66-4bf6-dd37-525d8b791a55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/user/1000/app/com.jetbrains.PyCharm-Community/ipykernel_10478/155079712.py:3: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
      "  df.style.background_gradient(cmap=cm).set_precision(2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_77eeb_row0_col0, #T_77eeb_row1_col1, #T_77eeb_row2_col2, #T_77eeb_row3_col3, #T_77eeb_row4_col4, #T_77eeb_row5_col5, #T_77eeb_row6_col6, #T_77eeb_row7_col7, #T_77eeb_row8_col8, #T_77eeb_row9_col9 {\n",
       "  background-color: #008000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_77eeb_row0_col1 {\n",
       "  background-color: #e1eee1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col2, #T_77eeb_row3_col9, #T_77eeb_row4_col0, #T_77eeb_row4_col5 {\n",
       "  background-color: #d6e9d6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col3, #T_77eeb_row1_col2, #T_77eeb_row1_col3, #T_77eeb_row1_col4, #T_77eeb_row1_col5, #T_77eeb_row1_col7, #T_77eeb_row4_col9, #T_77eeb_row6_col0, #T_77eeb_row6_col8, #T_77eeb_row7_col1, #T_77eeb_row7_col8, #T_77eeb_row8_col3, #T_77eeb_row8_col6, #T_77eeb_row8_col7, #T_77eeb_row9_col5 {\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col4, #T_77eeb_row6_col7 {\n",
       "  background-color: #e2efe2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col5, #T_77eeb_row1_col6, #T_77eeb_row5_col0, #T_77eeb_row5_col8, #T_77eeb_row5_col9, #T_77eeb_row6_col1, #T_77eeb_row8_col5 {\n",
       "  background-color: #e9f2e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col6, #T_77eeb_row8_col4 {\n",
       "  background-color: #e6f1e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col7, #T_77eeb_row2_col9, #T_77eeb_row9_col3 {\n",
       "  background-color: #e7f1e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col8 {\n",
       "  background-color: #afd6af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row0_col9, #T_77eeb_row7_col0, #T_77eeb_row7_col6, #T_77eeb_row9_col7 {\n",
       "  background-color: #e3efe3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row1_col0 {\n",
       "  background-color: #daebda;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row1_col8, #T_77eeb_row3_col2, #T_77eeb_row7_col3 {\n",
       "  background-color: #c3e0c3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row1_col9, #T_77eeb_row2_col3, #T_77eeb_row6_col2 {\n",
       "  background-color: #b4d8b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col0, #T_77eeb_row3_col7 {\n",
       "  background-color: #cbe4cb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col1, #T_77eeb_row2_col8, #T_77eeb_row3_col8, #T_77eeb_row9_col6 {\n",
       "  background-color: #e5f0e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col4 {\n",
       "  background-color: #a6d2a6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col5 {\n",
       "  background-color: #b6d9b6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col6 {\n",
       "  background-color: #d3e7d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row2_col7 {\n",
       "  background-color: #d4e8d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row3_col0, #T_77eeb_row7_col2, #T_77eeb_row7_col9 {\n",
       "  background-color: #deedde;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row3_col1, #T_77eeb_row4_col1 {\n",
       "  background-color: #e8f2e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row3_col4 {\n",
       "  background-color: #cfe5cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row3_col5 {\n",
       "  background-color: #8fc68f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row3_col6 {\n",
       "  background-color: #c6e1c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row4_col2 {\n",
       "  background-color: #71b771;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_77eeb_row4_col3 {\n",
       "  background-color: #c0dec0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row4_col6, #T_77eeb_row5_col2 {\n",
       "  background-color: #badbba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row4_col7, #T_77eeb_row9_col8 {\n",
       "  background-color: #b3d8b3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row4_col8, #T_77eeb_row5_col1, #T_77eeb_row8_col2, #T_77eeb_row9_col2, #T_77eeb_row9_col4 {\n",
       "  background-color: #eaf2ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row5_col3 {\n",
       "  background-color: #53a953;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_77eeb_row5_col4 {\n",
       "  background-color: #cee5ce;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row5_col6, #T_77eeb_row6_col5 {\n",
       "  background-color: #dfeddf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row5_col7, #T_77eeb_row7_col4 {\n",
       "  background-color: #bcdcbc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row6_col3 {\n",
       "  background-color: #a7d2a7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row6_col4 {\n",
       "  background-color: #abd4ab;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row6_col9 {\n",
       "  background-color: #ddecdd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row7_col5 {\n",
       "  background-color: #c2dfc2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row8_col0 {\n",
       "  background-color: #b9dbb9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row8_col1, #T_77eeb_row9_col0 {\n",
       "  background-color: #dbebdb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row8_col9 {\n",
       "  background-color: #e0eedf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_77eeb_row9_col1 {\n",
       "  background-color: #b7dab7;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_77eeb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_77eeb_level0_col0\" class=\"col_heading level0 col0\" >plane</th>\n",
       "      <th id=\"T_77eeb_level0_col1\" class=\"col_heading level0 col1\" >car</th>\n",
       "      <th id=\"T_77eeb_level0_col2\" class=\"col_heading level0 col2\" >bird</th>\n",
       "      <th id=\"T_77eeb_level0_col3\" class=\"col_heading level0 col3\" >cat</th>\n",
       "      <th id=\"T_77eeb_level0_col4\" class=\"col_heading level0 col4\" >deer</th>\n",
       "      <th id=\"T_77eeb_level0_col5\" class=\"col_heading level0 col5\" >dog</th>\n",
       "      <th id=\"T_77eeb_level0_col6\" class=\"col_heading level0 col6\" >frog</th>\n",
       "      <th id=\"T_77eeb_level0_col7\" class=\"col_heading level0 col7\" >horse</th>\n",
       "      <th id=\"T_77eeb_level0_col8\" class=\"col_heading level0 col8\" >ship</th>\n",
       "      <th id=\"T_77eeb_level0_col9\" class=\"col_heading level0 col9\" >truck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row0\" class=\"row_heading level0 row0\" >plane</th>\n",
       "      <td id=\"T_77eeb_row0_col0\" class=\"data row0 col0\" >624</td>\n",
       "      <td id=\"T_77eeb_row0_col1\" class=\"data row0 col1\" >32</td>\n",
       "      <td id=\"T_77eeb_row0_col2\" class=\"data row0 col2\" >39</td>\n",
       "      <td id=\"T_77eeb_row0_col3\" class=\"data row0 col3\" >17</td>\n",
       "      <td id=\"T_77eeb_row0_col4\" class=\"data row0 col4\" >9</td>\n",
       "      <td id=\"T_77eeb_row0_col5\" class=\"data row0 col5\" >6</td>\n",
       "      <td id=\"T_77eeb_row0_col6\" class=\"data row0 col6\" >15</td>\n",
       "      <td id=\"T_77eeb_row0_col7\" class=\"data row0 col7\" >20</td>\n",
       "      <td id=\"T_77eeb_row0_col8\" class=\"data row0 col8\" >208</td>\n",
       "      <td id=\"T_77eeb_row0_col9\" class=\"data row0 col9\" >30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row1\" class=\"row_heading level0 row1\" >car</th>\n",
       "      <td id=\"T_77eeb_row1_col0\" class=\"data row1 col0\" >60</td>\n",
       "      <td id=\"T_77eeb_row1_col1\" class=\"data row1 col1\" >615</td>\n",
       "      <td id=\"T_77eeb_row1_col2\" class=\"data row1 col2\" >3</td>\n",
       "      <td id=\"T_77eeb_row1_col3\" class=\"data row1 col3\" >18</td>\n",
       "      <td id=\"T_77eeb_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_77eeb_row1_col5\" class=\"data row1 col5\" >2</td>\n",
       "      <td id=\"T_77eeb_row1_col6\" class=\"data row1 col6\" >9</td>\n",
       "      <td id=\"T_77eeb_row1_col7\" class=\"data row1 col7\" >12</td>\n",
       "      <td id=\"T_77eeb_row1_col8\" class=\"data row1 col8\" >150</td>\n",
       "      <td id=\"T_77eeb_row1_col9\" class=\"data row1 col9\" >131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row2\" class=\"row_heading level0 row2\" >bird</th>\n",
       "      <td id=\"T_77eeb_row2_col0\" class=\"data row2 col0\" >98</td>\n",
       "      <td id=\"T_77eeb_row2_col1\" class=\"data row2 col1\" >23</td>\n",
       "      <td id=\"T_77eeb_row2_col2\" class=\"data row2 col2\" >418</td>\n",
       "      <td id=\"T_77eeb_row2_col3\" class=\"data row2 col3\" >105</td>\n",
       "      <td id=\"T_77eeb_row2_col4\" class=\"data row2 col4\" >73</td>\n",
       "      <td id=\"T_77eeb_row2_col5\" class=\"data row2 col5\" >81</td>\n",
       "      <td id=\"T_77eeb_row2_col6\" class=\"data row2 col6\" >61</td>\n",
       "      <td id=\"T_77eeb_row2_col7\" class=\"data row2 col7\" >69</td>\n",
       "      <td id=\"T_77eeb_row2_col8\" class=\"data row2 col8\" >49</td>\n",
       "      <td id=\"T_77eeb_row2_col9\" class=\"data row2 col9\" >23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row3\" class=\"row_heading level0 row3\" >cat</th>\n",
       "      <td id=\"T_77eeb_row3_col0\" class=\"data row3 col0\" >50</td>\n",
       "      <td id=\"T_77eeb_row3_col1\" class=\"data row3 col1\" >16</td>\n",
       "      <td id=\"T_77eeb_row3_col2\" class=\"data row3 col2\" >74</td>\n",
       "      <td id=\"T_77eeb_row3_col3\" class=\"data row3 col3\" >398</td>\n",
       "      <td id=\"T_77eeb_row3_col4\" class=\"data row3 col4\" >30</td>\n",
       "      <td id=\"T_77eeb_row3_col5\" class=\"data row3 col5\" >139</td>\n",
       "      <td id=\"T_77eeb_row3_col6\" class=\"data row3 col6\" >90</td>\n",
       "      <td id=\"T_77eeb_row3_col7\" class=\"data row3 col7\" >95</td>\n",
       "      <td id=\"T_77eeb_row3_col8\" class=\"data row3 col8\" >50</td>\n",
       "      <td id=\"T_77eeb_row3_col9\" class=\"data row3 col9\" >58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row4\" class=\"row_heading level0 row4\" >deer</th>\n",
       "      <td id=\"T_77eeb_row4_col0\" class=\"data row4 col0\" >70</td>\n",
       "      <td id=\"T_77eeb_row4_col1\" class=\"data row4 col1\" >16</td>\n",
       "      <td id=\"T_77eeb_row4_col2\" class=\"data row4 col2\" >218</td>\n",
       "      <td id=\"T_77eeb_row4_col3\" class=\"data row4 col3\" >86</td>\n",
       "      <td id=\"T_77eeb_row4_col4\" class=\"data row4 col4\" >250</td>\n",
       "      <td id=\"T_77eeb_row4_col5\" class=\"data row4 col5\" >33</td>\n",
       "      <td id=\"T_77eeb_row4_col6\" class=\"data row4 col6\" >118</td>\n",
       "      <td id=\"T_77eeb_row4_col7\" class=\"data row4 col7\" >159</td>\n",
       "      <td id=\"T_77eeb_row4_col8\" class=\"data row4 col8\" >36</td>\n",
       "      <td id=\"T_77eeb_row4_col9\" class=\"data row4 col9\" >14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row5\" class=\"row_heading level0 row5\" >dog</th>\n",
       "      <td id=\"T_77eeb_row5_col0\" class=\"data row5 col0\" >23</td>\n",
       "      <td id=\"T_77eeb_row5_col1\" class=\"data row5 col1\" >11</td>\n",
       "      <td id=\"T_77eeb_row5_col2\" class=\"data row5 col2\" >89</td>\n",
       "      <td id=\"T_77eeb_row5_col3\" class=\"data row5 col3\" >264</td>\n",
       "      <td id=\"T_77eeb_row5_col4\" class=\"data row5 col4\" >31</td>\n",
       "      <td id=\"T_77eeb_row5_col5\" class=\"data row5 col5\" >354</td>\n",
       "      <td id=\"T_77eeb_row5_col6\" class=\"data row5 col6\" >33</td>\n",
       "      <td id=\"T_77eeb_row5_col7\" class=\"data row5 col7\" >137</td>\n",
       "      <td id=\"T_77eeb_row5_col8\" class=\"data row5 col8\" >40</td>\n",
       "      <td id=\"T_77eeb_row5_col9\" class=\"data row5 col9\" >18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row6\" class=\"row_heading level0 row6\" >frog</th>\n",
       "      <td id=\"T_77eeb_row6_col0\" class=\"data row6 col0\" >16</td>\n",
       "      <td id=\"T_77eeb_row6_col1\" class=\"data row6 col1\" >13</td>\n",
       "      <td id=\"T_77eeb_row6_col2\" class=\"data row6 col2\" >99</td>\n",
       "      <td id=\"T_77eeb_row6_col3\" class=\"data row6 col3\" >127</td>\n",
       "      <td id=\"T_77eeb_row6_col4\" class=\"data row6 col4\" >68</td>\n",
       "      <td id=\"T_77eeb_row6_col5\" class=\"data row6 col5\" >20</td>\n",
       "      <td id=\"T_77eeb_row6_col6\" class=\"data row6 col6\" >547</td>\n",
       "      <td id=\"T_77eeb_row6_col7\" class=\"data row6 col7\" >33</td>\n",
       "      <td id=\"T_77eeb_row6_col8\" class=\"data row6 col8\" >32</td>\n",
       "      <td id=\"T_77eeb_row6_col9\" class=\"data row6 col9\" >45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row7\" class=\"row_heading level0 row7\" >horse</th>\n",
       "      <td id=\"T_77eeb_row7_col0\" class=\"data row7 col0\" >35</td>\n",
       "      <td id=\"T_77eeb_row7_col1\" class=\"data row7 col1\" >8</td>\n",
       "      <td id=\"T_77eeb_row7_col2\" class=\"data row7 col2\" >27</td>\n",
       "      <td id=\"T_77eeb_row7_col3\" class=\"data row7 col3\" >81</td>\n",
       "      <td id=\"T_77eeb_row7_col4\" class=\"data row7 col4\" >50</td>\n",
       "      <td id=\"T_77eeb_row7_col5\" class=\"data row7 col5\" >63</td>\n",
       "      <td id=\"T_77eeb_row7_col6\" class=\"data row7 col6\" >23</td>\n",
       "      <td id=\"T_77eeb_row7_col7\" class=\"data row7 col7\" >639</td>\n",
       "      <td id=\"T_77eeb_row7_col8\" class=\"data row7 col8\" >32</td>\n",
       "      <td id=\"T_77eeb_row7_col9\" class=\"data row7 col9\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row8\" class=\"row_heading level0 row8\" >ship</th>\n",
       "      <td id=\"T_77eeb_row8_col0\" class=\"data row8 col0\" >145</td>\n",
       "      <td id=\"T_77eeb_row8_col1\" class=\"data row8 col1\" >50</td>\n",
       "      <td id=\"T_77eeb_row8_col2\" class=\"data row8 col2\" >6</td>\n",
       "      <td id=\"T_77eeb_row8_col3\" class=\"data row8 col3\" >18</td>\n",
       "      <td id=\"T_77eeb_row8_col4\" class=\"data row8 col4\" >5</td>\n",
       "      <td id=\"T_77eeb_row8_col5\" class=\"data row8 col5\" >5</td>\n",
       "      <td id=\"T_77eeb_row8_col6\" class=\"data row8 col6\" >4</td>\n",
       "      <td id=\"T_77eeb_row8_col7\" class=\"data row8 col7\" >10</td>\n",
       "      <td id=\"T_77eeb_row8_col8\" class=\"data row8 col8\" >719</td>\n",
       "      <td id=\"T_77eeb_row8_col9\" class=\"data row8 col9\" >38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_77eeb_level0_row9\" class=\"row_heading level0 row9\" >truck</th>\n",
       "      <td id=\"T_77eeb_row9_col0\" class=\"data row9 col0\" >58</td>\n",
       "      <td id=\"T_77eeb_row9_col1\" class=\"data row9 col1\" >143</td>\n",
       "      <td id=\"T_77eeb_row9_col2\" class=\"data row9 col2\" >6</td>\n",
       "      <td id=\"T_77eeb_row9_col3\" class=\"data row9 col3\" >24</td>\n",
       "      <td id=\"T_77eeb_row9_col4\" class=\"data row9 col4\" >1</td>\n",
       "      <td id=\"T_77eeb_row9_col5\" class=\"data row9 col5\" >2</td>\n",
       "      <td id=\"T_77eeb_row9_col6\" class=\"data row9 col6\" >18</td>\n",
       "      <td id=\"T_77eeb_row9_col7\" class=\"data row9 col7\" >32</td>\n",
       "      <td id=\"T_77eeb_row9_col8\" class=\"data row9 col8\" >195</td>\n",
       "      <td id=\"T_77eeb_row9_col9\" class=\"data row9 col9\" >521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3a0c7fbf10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "df.style.background_gradient(cmap=cm).set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hktUgjMojF5O"
   },
   "source": [
    "**Ejercicio:** Analizar los resultados anteriores, ¿vale la pena hacer algunos cambios al modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5Mk56NcjF5O"
   },
   "source": [
    "## CNNs para Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMY_yMSjF5O"
   },
   "source": [
    "### Datos IMDB\n",
    "\n",
    "Similar al caso de CNN para imágenes, vamos a volver sobre el conjunto de datos que ya utilizamos anteriomente: el de reviews IMDB. Esta vez para compararlo contra el modelo de perceptrón multicapa utilizando la media de los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hXzTsuEtjF5O"
   },
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "        \n",
    "        item = {\n",
    "            \"data\": self.dataset.loc[item, \"review\"],\n",
    "            \"target\": self.dataset.loc[item, \"sentiment\"]\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf6ZhstEjF5O"
   },
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "Aplicamos el mismo tipo de preprocesamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8dwS_tZjF5P"
   },
   "source": [
    "**Algunas funciones auxiliares:**\n",
    "-  [`gensim.parsing.preprocessing.preprocess_string`](https://radimrehurek.com/gensim/parsing/preprocessing.html#:~:text=gensim.parsing.preprocessing.preprocess_string(s%2C%20filters%3D%5B%3Cfunction%20%3Clambda%3E%3E%2C%20%3Cfunction%20strip_tags%3E%2C%20%3Cfunction%20strip_punctuation%3E%2C%20%3Cfunction%20strip_multiple_whitespaces%3E%2C%20%3Cfunction%20strip_numeric%3E%2C%20%3Cfunction%20remove_stopwords%3E%2C%20%3Cfunction%20strip_short%3E%2C%20%3Cfunction%20stem_text%3E%5D)): Apply list of chosen filters to a string.\n",
    "\n",
    "- [`corpora.Dictionary`](https://radimrehurek.com/gensim/corpora/dictionary.html#:~:text=word%3C%2D%3Eid%20mappings-,corpora.dictionary%20%E2%80%93%20Construct%20word%3C%2D%3Eid%20mappings,of%20a%20Dictionary%20%E2%80%93%20a%20mapping%20between%20words%20and%20their%20integer%20ids.,-class): This module implements the concept of a Dictionary – a mapping between words and their integer ids.\n",
    "\n",
    "-  [`Dictionary.filter_extremes`](https://radimrehurek.com/gensim/corpora/dictionary.html?highlight=filter_extremes#gensim.corpora.dictionary.Dictionary.filter_extremes:~:text=filter_extremes(no_below%3D5%2C%20no_above%3D0.5%2C%20keep_n%3D100000%2C%20keep_tokens%3DNone))\n",
    "\n",
    "-  [`Dictionary.compactify`](https://radimrehurek.com/gensim/corpora/dictionary.html?highlight=compactify#gensim.corpora.dictionary.Dictionary.compactify:~:text=compactify(),shrinking%20any%20gaps.)\n",
    "\n",
    "- [`Dictionary.patch_with_special_tokens`](https://radimrehurek.com/gensim/corpora/dictionary.html?highlight=patch_with_special_tokens#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens:~:text=patch_with_special_tokens(special_token_dict)): Patch token2id and id2token using a dictionary of special tokens.\n",
    "\n",
    "- [`Dictionary.doc2idx`](https://radimrehurek.com/gensim/corpora/dictionary.html#:~:text=doc2idx(document%2C%20unknown_word_index%3D%2D1)): Convert document (a list of words) into a list of indexes = list of token_id. Replace all unknown words i.e, words not in the dictionary with the index as set via unknown_word_index.\n",
    "\n",
    "- [`__call__`](https://www.geeksforgeeks.org/__call__-in-python/): Permite que las instancias de una clase se comporten como funciones y puedan ser llamadas como funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YXiuU6h7jF5P"
   },
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 ignore_header=True, \n",
    "                 filters=None, \n",
    "                 vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [ #We set some filters\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing and filters)\n",
    "        # The dictionary has idx as a key and word as a value\n",
    "        # For example one element could be (0, 'accustomed') or (1, 'agenda')\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset[\"review\"].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        \n",
    "        # Filter the dictionary and compactify it (make the indices continous)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({\n",
    "            \"[PAD]\": 0, #The padding token\n",
    "            \"[UNK]\": 1  # The unknown token\n",
    "        })\n",
    "        \n",
    "        self.idx_to_target = sorted(dataset[\"sentiment\"].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "        \n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "    \n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "    \n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        #Encodeamos tanto los datos como los targets, diferenciandos los casos cuando son strings y cuando no\n",
    "        if isinstance(item[\"data\"], str): \n",
    "            data = self.encode_data(item[\"data\"])\n",
    "        else:\n",
    "            data = [self.encode_data(d) for d in item[\"data\"]]\n",
    "        \n",
    "        if isinstance(item[\"target\"], str):\n",
    "            target = self.encode_target(item[\"target\"])\n",
    "        else:\n",
    "            target = [self.encode_target(t) for t in item[\"target\"]]\n",
    "        \n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"target\": target\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYu-BdzqjF5P"
   },
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g__PSAcajF5Q",
    "outputId": "fe19b06e-f3f3-48fa-a8d7-e8446696d446"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41033</th>\n",
       "      <td>When it was announced the \"King of Pop\" was de...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48582</th>\n",
       "      <td>Not every movie with lesbian chicks and vampir...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16282</th>\n",
       "      <td>In keeping with Disney's well-known practice o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34524</th>\n",
       "      <td>I went to see this with my wife and 3 yr old s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12096</th>\n",
       "      <td>This movie is a window on the world of Britain...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "41033  When it was announced the \"King of Pop\" was de...  positive\n",
       "48582  Not every movie with lesbian chicks and vampir...  negative\n",
       "16282  In keeping with Disney's well-known practice o...  negative\n",
       "34524  I went to see this with my wife and 3 yr old s...  negative\n",
       "12096  This movie is a window on the world of Britain...  negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./data/imdb_reviews.csv.gz\")\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HWn-ppyQjF5Q"
   },
   "outputs": [],
   "source": [
    "preprocess = RawDataProcessor(dataset)\n",
    "#Separamos los datos de entrenamiento y test e instanciamos ambos conjuntos\n",
    "train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=42)\n",
    "train_dataset = IMDBReviewsDataset(dataset.loc[train_indices].reset_index(drop=True), \n",
    "                                   transform=preprocess)\n",
    "test_dataset = IMDBReviewsDataset(dataset.loc[test_indices].reset_index(drop=True), \n",
    "                                  transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Wn4pzM6ZjF5Q",
    "outputId": "858bb51e-8031-4450-c1d0-f06ba5d24f90"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15517</th>\n",
       "      <td>This film is harmless escapist fun. Something ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15229</th>\n",
       "      <td>I read the comment of Chris_m_grant from Unite...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>Let me just start out by saying that Tourist T...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828</th>\n",
       "      <td>*THIS REVIEW MAY CONTAIN SPOILERS... OR MAYBE ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29010</th>\n",
       "      <td>Duchess is a pretty white cat who lives with h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17680</th>\n",
       "      <td>From director Barbet Schroder (Reversal of For...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35141</th>\n",
       "      <td>After high-school graduation, best friends Ali...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>\"The Godfather\", \"Citizen Kane\", \"Star Wars\", ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30241</th>\n",
       "      <td>I rented this type of \"soft core\" before, but ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25259</th>\n",
       "      <td>Greystoke: The Legend of Tarzan, Lord of the A...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "15517  This film is harmless escapist fun. Something ...  positive\n",
       "15229  I read the comment of Chris_m_grant from Unite...  negative\n",
       "1389   Let me just start out by saying that Tourist T...  positive\n",
       "9828   *THIS REVIEW MAY CONTAIN SPOILERS... OR MAYBE ...  negative\n",
       "29010  Duchess is a pretty white cat who lives with h...  positive\n",
       "17680  From director Barbet Schroder (Reversal of For...  negative\n",
       "35141  After high-school graduation, best friends Ali...  positive\n",
       "8242   \"The Godfather\", \"Citizen Kane\", \"Star Wars\", ...  positive\n",
       "30241  I rented this type of \"soft core\" before, but ...  negative\n",
       "25259  Greystoke: The Legend of Tarzan, Lord of the A...  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v7d3l-ljF5R"
   },
   "source": [
    "### Padding de secuencias\n",
    "\n",
    "Dado que en este caso utilizaremos las secuencias completas sobre las que aplicaremos las convoluciones, necesitamos trabajar con dichas secuencias de manera que en un batch de datos tengan el tamaño correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YZ3AS-g3jF5R"
   },
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, \n",
    "                 pad_value=0, \n",
    "                 max_length=None, \n",
    "                 min_length=1):\n",
    "        \n",
    "        assert max_length is None or min_length <= max_length #Sanity check\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item[\"data\"], item[\"target\"]) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            # Si no tenemos max_lenght definido, tomamos el mínimo entre min_lenght y\n",
    "            # la longitud de la máxima secuencia\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "        \n",
    "        \n",
    "        # Para secuencias cuya longitud es menor que max_lenght rellenamos los valores\n",
    "        # faltantes con 0 (pad_value)\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l)\n",
    "                for d, l in zip(data, seq_lengths)]\n",
    "    \n",
    "        return {\n",
    "            \"data\": torch.LongTensor(data),\n",
    "            \"target\": torch.FloatTensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHn8X70ZjF5R"
   },
   "source": [
    "### DataLoaders\n",
    "\n",
    "Una vez creada nuestra función para hacer padding de secuencia, definiremos los `DataLoader`s. Una cuestión importante, las redes convolucionales sobre text esperan que todas las secuencias sean al menos del tamaño de la convolución máxima (caso contrario ocurrirá un error por no poder realizar la convolución sobre un espacio más chico que el tamaño de la convolución). Es por eso que utilizamos el parámetro `min_length` esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "siQCHjlxjF5R"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "FILTERS_COUNT = 100\n",
    "FILTERS_LENGTH = [2, 3, 4]\n",
    "\n",
    "#Instanciamos las clases\n",
    "pad_sequences = PadSequences(min_length=max(FILTERS_LENGTH))\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=128, \n",
    "                          shuffle=True,\n",
    "                          collate_fn=pad_sequences, #Aplicamos padding a las secuencias durante la creación del bath\n",
    "                          drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=128, \n",
    "                         shuffle=False,\n",
    "                         collate_fn=pad_sequences, #Aplicamos padding a las secuencias durante la creación del bath\n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y1R6MLwjF5R"
   },
   "source": [
    "### Red convolucional sobre texto\n",
    "\n",
    "Por último, tenemos la red convolucional sobre texto. Si bien arranca muy similar al caso del clasificador del perceptrón multicapa, vemos que en este caso hacemos uso de [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) dado que sólo nos desplazamos por una dimensión (i.e. la secuencia). En particular, como utilizamos *max pooling* global, no hacemos uso del módulo `torch.nn` para calcularlo, simplemente utilizamos el método `max()` del tensor.\n",
    "\n",
    "**Algunas funciones auxiliares:**\n",
    "- [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html): A simple lookup table that stores embeddings of a fixed dictionary and size.This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "- [`nn.Embedding.from_pretrained`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#:~:text=CLASSMETHOD%20from_pretrained(embeddings%2C%20freeze%3DTrue%2C%20padding_idx%3DNone%2C%20max_norm%3DNone%2C%20norm_type%3D2.0%2C%20scale_grad_by_freq%3DFalse%2C%20sparse%3DFalse)): Creates Embedding instance from given 2-dimensional FloatTensor.\n",
    "\n",
    "- [`torch.nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=modulelist#torch.nn.ModuleList:~:text=torch.nn.ModuleList(modules%3DNone)): Holds submodules in a list.\n",
    "\n",
    "- [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=torch%20cat#torch.cat:~:text=torch.cat(tensors%2C%20dim%3D0%2C%20*%2C%20out%3DNone)%20%E2%86%92%20Tensor): Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lSeA8sRsjF5S"
   },
   "outputs": [],
   "source": [
    "class IMDBReviewsClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_embeddings_path, \n",
    "                 dictionary,\n",
    "                 vector_size,\n",
    "                 freeze_embedings):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inicializamos la matriz de embeddings\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        \n",
    "        #Trabajamos con los embeddings preentrenados\n",
    "        with gzip.open(pretrained_embeddings_path, \"rt\") as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    embeddings_matrix[dictionary.token2id[word]] =\\\n",
    "                        torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        \n",
    "        # Los guardamos en la variable embeddings\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix,\n",
    "                                                       freeze=freeze_embedings,\n",
    "                                                       padding_idx=0)\n",
    "        self.convs = []\n",
    "        for filter_lenght in FILTERS_LENGTH:\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(vector_size, FILTERS_COUNT, filter_lenght) #(in_channels, out_channels, kernel_size)\n",
    "            )\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.fc = nn.Linear(FILTERS_COUNT * len(FILTERS_LENGTH), 128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.vector_size = vector_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_global_max_pool(x, conv):\n",
    "        return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x).transpose(1, 2)  \n",
    "        x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUKoGwKTjF5S"
   },
   "source": [
    "### Experimento\n",
    "\n",
    "El experimento de MLflow es prácticamente igual, salvo que cambiamos algunos de los parámetros a guardar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JEypLUdojF5S"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590c87b15e6a42c9a681269def28f43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e70ff88086c4b8f893d8e0f16c323a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637c623d9fa9440e9a32bb3048f99ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07124c4a75e74dfc969df9161e32f9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c840b212654b9ba308bf3c80713aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23212df5d00146cba24351bf968f05d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845f709b3ba740f78038e686f207e5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8288e8d5a8da4b9fa99a4cdb37a09c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"a_naive_experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_name\", \"cnn\")\n",
    "    mlflow.log_param(\"freeze_embedding\", True)\n",
    "    mlflow.log_params({\n",
    "        \"filters_count\": FILTERS_COUNT,\n",
    "        \"filters_length\": FILTERS_LENGTH,\n",
    "        \"fc_size\": 128\n",
    "    })\n",
    "    model = IMDBReviewsClassifier(\"./data/glove.6B.50d.txt.gz\", preprocess.dictionary, 50, True)\n",
    "    loss = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    for epoch in trange(3):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch[\"data\"])\n",
    "            loss_value = loss(output, batch[\"target\"].view(-1, 1))\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())        \n",
    "        mlflow.log_metric(\"train_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch[\"data\"])\n",
    "            running_loss.append(\n",
    "                loss(output, batch[\"target\"].view(-1, 1)).item()\n",
    "            )\n",
    "            targets.extend(batch[\"target\"].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        mlflow.log_metric(\"test_loss\", sum(running_loss) / len(running_loss), epoch)\n",
    "        mlflow.log_metric(\"test_avp\", average_precision_score(targets, predictions), epoch)\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch[\"data\"])\n",
    "            targets.extend(batch[\"target\"].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        pd.DataFrame({\"prediction\": predictions, \"target\": targets}).to_csv(\n",
    "            f\"{tmpdirname}/predictions.csv.gz\", index=False\n",
    "        )\n",
    "        mlflow.log_artifact(f\"{tmpdirname}/predictions.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sTTKflqgjF5T",
    "outputId": "ccfa8e16-6419-497c-896f-128cbc05ac16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDBReviewsClassifier(\n",
      "  (embeddings): Embedding(50002, 50, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(50, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(50, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=300, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mcMlwFf0jF5T"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='file:///home/adrian/PycharmProjects/AprendizajeProfundo/mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>,\n",
       " <Experiment: artifact_location='file:///home/adrian/PycharmProjects/AprendizajeProfundo/mlruns/1', experiment_id='1', lifecycle_stage='active', name='a_naive_experiment', tags={}>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.tracking.MlflowClient().list_experiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vhisTKDRjF5T"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.test_loss</th>\n",
       "      <th>metrics.test_avp</th>\n",
       "      <th>metrics.train_loss</th>\n",
       "      <th>params.filters_length</th>\n",
       "      <th>...</th>\n",
       "      <th>params.fc_size</th>\n",
       "      <th>params.freeze_embedding</th>\n",
       "      <th>params.model_name</th>\n",
       "      <th>params.hidden1_size</th>\n",
       "      <th>params.embedding_size</th>\n",
       "      <th>params.hidden2_size</th>\n",
       "      <th>tags.mlflow.source.git.commit</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4904522016014aa6ba867ddb32df0646</td>\n",
       "      <td>1</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///home/adrian/PycharmProjects/Aprendizaj...</td>\n",
       "      <td>2022-10-26 04:11:23.156000+00:00</td>\n",
       "      <td>2022-10-26 04:13:49.300000+00:00</td>\n",
       "      <td>0.337537</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.286678</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>True</td>\n",
       "      <td>cnn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1949c7ec3247aed4dd83cc64c892fcc39628c612</td>\n",
       "      <td>/home/adrian/PycharmProjects/AprendizajeProfun...</td>\n",
       "      <td>adrian</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c1a56f32f7604db4a4bf4790e53215f1</td>\n",
       "      <td>1</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///home/adrian/PycharmProjects/Aprendizaj...</td>\n",
       "      <td>2022-10-26 03:57:43.985000+00:00</td>\n",
       "      <td>2022-10-26 03:58:25.528000+00:00</td>\n",
       "      <td>0.513786</td>\n",
       "      <td>0.829681</td>\n",
       "      <td>0.510267</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>128</td>\n",
       "      <td>1949c7ec3247aed4dd83cc64c892fcc39628c612</td>\n",
       "      <td>/home/adrian/PycharmProjects/AprendizajeProfun...</td>\n",
       "      <td>adrian</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2117e91ce6d413ab89925c7faaf8117</td>\n",
       "      <td>1</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///home/adrian/PycharmProjects/Aprendizaj...</td>\n",
       "      <td>2022-09-10 16:27:21.074000+00:00</td>\n",
       "      <td>2022-09-10 16:28:40.887000+00:00</td>\n",
       "      <td>0.511009</td>\n",
       "      <td>0.830107</td>\n",
       "      <td>0.509006</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>128</td>\n",
       "      <td>79a6fdd8b00d19f817b3ded7f50f5ce2d1ab1627</td>\n",
       "      <td>/home/adrian/PycharmProjects/AprendizajeProfun...</td>\n",
       "      <td>adrian</td>\n",
       "      <td>LOCAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id experiment_id    status  \\\n",
       "0  4904522016014aa6ba867ddb32df0646             1  FINISHED   \n",
       "1  c1a56f32f7604db4a4bf4790e53215f1             1  FINISHED   \n",
       "2  e2117e91ce6d413ab89925c7faaf8117             1  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  file:///home/adrian/PycharmProjects/Aprendizaj...   \n",
       "1  file:///home/adrian/PycharmProjects/Aprendizaj...   \n",
       "2  file:///home/adrian/PycharmProjects/Aprendizaj...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2022-10-26 04:11:23.156000+00:00 2022-10-26 04:13:49.300000+00:00   \n",
       "1 2022-10-26 03:57:43.985000+00:00 2022-10-26 03:58:25.528000+00:00   \n",
       "2 2022-09-10 16:27:21.074000+00:00 2022-09-10 16:28:40.887000+00:00   \n",
       "\n",
       "   metrics.test_loss  metrics.test_avp  metrics.train_loss  \\\n",
       "0           0.337537          0.926230            0.286678   \n",
       "1           0.513786          0.829681            0.510267   \n",
       "2           0.511009          0.830107            0.509006   \n",
       "\n",
       "  params.filters_length  ... params.fc_size params.freeze_embedding  \\\n",
       "0             [2, 3, 4]  ...            128                    True   \n",
       "1                  None  ...           None                    True   \n",
       "2                  None  ...           None                    True   \n",
       "\n",
       "  params.model_name params.hidden1_size params.embedding_size  \\\n",
       "0               cnn                None                  None   \n",
       "1               mlp                 128                    50   \n",
       "2               mlp                 128                    50   \n",
       "\n",
       "  params.hidden2_size             tags.mlflow.source.git.commit  \\\n",
       "0                None  1949c7ec3247aed4dd83cc64c892fcc39628c612   \n",
       "1                 128  1949c7ec3247aed4dd83cc64c892fcc39628c612   \n",
       "2                 128  79a6fdd8b00d19f817b3ded7f50f5ce2d1ab1627   \n",
       "\n",
       "                             tags.mlflow.source.name tags.mlflow.user  \\\n",
       "0  /home/adrian/PycharmProjects/AprendizajeProfun...           adrian   \n",
       "1  /home/adrian/PycharmProjects/AprendizajeProfun...           adrian   \n",
       "2  /home/adrian/PycharmProjects/AprendizajeProfun...           adrian   \n",
       "\n",
       "  tags.mlflow.source.type  \n",
       "0                   LOCAL  \n",
       "1                   LOCAL  \n",
       "2                   LOCAL  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.search_runs().head(5)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
